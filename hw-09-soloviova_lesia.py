# -*- coding: utf-8 -*-
"""goit-numericalpy-hw-09-soloviova_lesia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CT3yZkBP2aW9_5Yi8eyRNc_Z3Y6fW-1O
"""

import gym
import numpy as np
import matplotlib.pyplot as plt

# Функція для відображення зображення
def show_render(img):
    """
    Відображає зображення середовища.
    Обробляє випадки, коли зображення є списком або має зайві виміри.
    """
    if isinstance(img, list):
        img = img[0]  # Вибираємо перше зображення зі списку

    if img.ndim == 4:
        img = img[0]  # Якщо зображення є батчем, вибираємо перший елемент

    img = np.squeeze(img)  # Видаляємо зайві виміри
    if img.ndim == 3 and img.shape[-1] == 3:  # Якщо зображення має канали RGB
        img = img[..., :3]  # Вибираємо тільки перші три канали (RGB)

    plt.imshow(img)
    plt.axis('off')  # Вимкнути вісі
    plt.show()

# Створення середовища FrozenLake-v1
def create_environment():
    """
    Створює та повертає середовище FrozenLake-v1.
    """
    env = gym.make(
        'FrozenLake-v1',
        desc=None,
        map_name="4x4",
        is_slippery=True,
        render_mode="rgb_array",
        new_step_api=True
    )
    return env

# Функція для вибору випадкових дій
def random_policy(env):
    """
    Повертає випадкову дію з простору дій середовища.
    """
    return np.random.choice(env.action_space.n)

# Функція ітерації за цінністю
def value_iteration(env, gamma=1.0, threshold=1e-20, max_iterations=100000):
    """
    Виконує ітерацію за цінністю для знаходження оптимальної функції цінності.
    """
    value_table = np.zeros(env.observation_space.n)

    for i in range(max_iterations):
        updated_value_table = np.copy(value_table)

        for state in range(env.observation_space.n):
            Q_value = [
                sum(
                    trans_prob * (reward_prob + gamma * updated_value_table[next_state])
                    for trans_prob, next_state, reward_prob, _ in env.P[state][action]
                )
                for action in range(env.action_space.n)
            ]
            value_table[state] = max(Q_value)

        if np.sum(np.fabs(updated_value_table - value_table)) <= threshold:
            print(f'Value-iteration converged at iteration# {i + 1}.')
            break

    return value_table

# Функція отримання політики з функції цінності
def extract_policy(value_table, gamma=1.0):
    """
    Вилучає оптимальну політику з функції цінності.
    """
    policy = np.zeros(env.observation_space.n, dtype=int)

    for state in range(env.observation_space.n):
        Q_table = [
            sum(
                trans_prob * (reward_prob + gamma * value_table[next_state])
                for trans_prob, next_state, reward_prob, _ in env.P[state][action]
            )
            for action in range(env.action_space.n)
        ]
        policy[state] = np.argmax(Q_table)

    return policy

# Функція ітерації за політиками
def policy_iteration(env, gamma=1.0, threshold=1e-10, max_iterations=100000):
    """
    Виконує ітерацію за політиками для знаходження оптимальної політики.
    """
    def compute_value_function(policy, gamma):
        value_table = np.zeros(env.observation_space.n)

        while True:
            updated_value_table = np.copy(value_table)

            for state in range(env.observation_space.n):
                action = policy[state]
                value_table[state] = sum(
                    trans_prob * (reward_prob + gamma * updated_value_table[next_state])
                    for trans_prob, next_state, reward_prob, _ in env.P[state][action]
                )

            if np.sum(np.fabs(updated_value_table - value_table)) <= threshold:
                break

        return value_table

    policy = np.zeros(env.observation_space.n, dtype=int)

    for i in range(max_iterations):
        new_value_function = compute_value_function(policy, gamma)
        new_policy = extract_policy(new_value_function, gamma)

        if np.array_equal(policy, new_policy):
            print(f'Policy-iteration converged at iteration# {i + 1}.')
            break

        policy = new_policy

    return policy

# Функція для виконання політики в середовищі
def run_policy(env, policy):
    """
    Виконує політику в середовищі та візуалізує процес.
    """
    obs = env.reset()  # Ініціалізація середовища
    done = False
    total_reward = 0

    while not done:
        action = int(policy[obs])  # Вибір дії з політики
        obs, reward, done, truncated, info = env.step(action)  # Виконання дії
        total_reward += reward  # Додавання винагороди

        img = env.render()  # Отримуємо зображення
        show_render(img)  # Відображаємо зображення

        if done:
            print(f"Гра завершена! Оцінка: {total_reward}")
            break

# Основна частина програми
if __name__ == "__main__":
    # Створення середовища
    env = create_environment()

    # Візуалізація початкового стану середовища
    print("\n=== Початковий стан середовища ===")
    obs = env.reset()
    start_img = env.render()
    show_render(start_img)

    # Виконання кількох кроків з випадковими діями
    print("\n=== Виконання випадкових дій ===")
    for _ in range(20):  # Виконати 20 кроків
        action = random_policy(env)
        obs, reward, done, truncated, info = env.step(action)
        img = env.render()
        show_render(img)
        if done:
            break

    # Ітерація за цінністю
    print("\n=== Ітерація за цінністю ===")
    optimal_value_function = value_iteration(env=env, gamma=1.0)
    optimal_policy_vi = extract_policy(optimal_value_function, gamma=1.0)
    print("Оптимальна функція цінності:")
    print(optimal_value_function)
    print("Оптимальна політика:")
    print(optimal_policy_vi)

    # Візуалізація після ітерації за цінністю
    print("\n=== Середовище після ітерації за цінністю ===")
    obs = env.reset()
    value_img = env.render()
    show_render(value_img)

    # Ітерація за політиками
    print("\n=== Ітерація за політиками ===")
    optimal_policy_pi = policy_iteration(env=env, gamma=1.0)
    print("Оптимальна політика:")
    print(optimal_policy_pi)

    # Візуалізація після ітерації за політиками
    print("\n=== Середовище після ітерації за політиками ===")
    obs = env.reset()
    policy_img = env.render()
    show_render(policy_img)

    # Виконання оптимальної політики
    print("\n=== Виконання оптимальної політики ===")
    run_policy(env, optimal_policy_pi)

    # Висновки
    print("\n=== Висновки ===")
    import gym
import numpy as np
import matplotlib.pyplot as plt

def show_render(img):
    if isinstance(img, list):
        img = img[0]
    if img.ndim == 4:
        img = img[0]
    img = np.squeeze(img)
    if img.ndim == 3 and img.shape[-1] == 3:
        img = img[..., :3]
    plt.imshow(img)
    plt.axis('off')
    plt.show()

def compute_value_function(env, gamma=1.0, threshold=1e-20, max_iterations=100000):
    print("Виконується ітерація за цінністю...")
    value_table = np.zeros(env.observation_space.n)

    for i in range(max_iterations):
        updated_value_table = np.copy(value_table)

        for state in range(env.observation_space.n):
            Q_value = []
            for action in range(env.action_space.n):
                next_states_rewards = []
                for next_sr in env.P[state][action]:
                    trans_prob, next_state, reward_prob, _ = next_sr
                    next_states_rewards.append(
                        trans_prob * (reward_prob + gamma * updated_value_table[next_state])
                    )
                Q_value.append(np.sum(next_states_rewards))
            value_table[state] = max(Q_value)

        if np.sum(np.fabs(updated_value_table - value_table)) <= threshold:
            print(f'Value-iteration converged at iteration# {i+1}.')
            break

    print("Завершено ітерацію за цінністю")
    return value_table

def policy_iteration(env, gamma=1.0, max_iterations=10000):
    print("Виконується ітерація за політиками...")
    policy = np.zeros(env.observation_space.n, dtype=int)
    value_table = np.zeros(env.observation_space.n)

    for i in range(max_iterations):
        while True:
            updated_value_table = np.copy(value_table)
            for state in range(env.observation_space.n):
                action = policy[state]
                value_table[state] = sum(
                    trans_prob * (reward + gamma * updated_value_table[next_state])
                    for trans_prob, next_state, reward, _ in env.P[state][action]
                )
            if np.sum(np.fabs(updated_value_table - value_table)) < 1e-20:
                break

        policy_stable = True
        for state in range(env.observation_space.n):
            old_action = policy[state]
            Q_table = np.zeros(env.action_space.n)
            for action in range(env.action_space.n):
                Q_table[action] = sum(
                    trans_prob * (reward + gamma * value_table[next_state])
                    for trans_prob, next_state, reward, _ in env.P[state][action]
                )
            policy[state] = np.argmax(Q_table)
            if old_action != policy[state]:
                policy_stable = False

        if policy_stable:
            print(f'Policy-iteration converged at iteration# {i+1}.')
            break

    print("Завершено ітерацію за політиками")
    return policy

# Завантаження середовища
print("Завантаження середовища FrozenLake-v1")
env = gym.make('FrozenLake-v1', is_slippery=True, render_mode="rgb_array")

# Скидання середовища перед візуалізацією
env.reset()

# Виконання ітерацій за політикою
optimal_value_function = compute_value_function(env, gamma=1.0)
optimal_policy = policy_iteration(env, gamma=1.0)

# Вивід результатів у консоль
print("Optimal Value Function:")
print(optimal_value_function)
print("Optimal Policy:")
print(optimal_policy)

# Візуалізація результату
show_render(env.render())

# Висновки
print("\n=== Висновки ===\n"
      "1. Ітерація за цінністю є ефективною, оскільки вона безпосередньо оновлює функцію цінності без необхідності збереження політики.\n"
      "2. Ітерація за політиками дозволяє швидко адаптуватися до змін середовища, проте вона може вимагати більше ітерацій для досягнення збіжності.\n"
      "3. Для завдань з невеликою кількістю станів обидва методи дають однаковий результат, проте для великих просторів станів ітерація за цінністю зазвичай швидше досягає оптимального рішення.")